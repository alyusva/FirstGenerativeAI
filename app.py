import streamlit as st
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling
import os
import PyPDF2  

# Configurar la aplicaci√≥n
st.title("üöÄ Mi primera aplicaci√≥n de IA Generativa en Espa√±ol con GPT-2")

# Cargar el modelo en espa√±ol
MODEL_NAME = "DeepESP/gpt2-spanish"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model_path = "gpt2_finetuned"

if os.path.exists(model_path):
    model = AutoModelForCausalLM.from_pretrained(model_path).to("cpu")
    st.write("‚úÖ **Modelo cargado desde el entrenamiento previo.**")
else:
    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to("cpu")
    st.write("üîÑ **Modelo base en espa√±ol cargado.**")

# Funci√≥n para extraer texto de un archivo PDF
def extract_text_from_pdf(pdf_file):
    pdf_reader = PyPDF2.PdfReader(pdf_file)
    text = ""
    for page in pdf_reader.pages:
        text += page.extract_text() + "\n"
    return text

# **Secci√≥n para subir archivos PDF o TXT**
st.subheader("üìÇ Subir un archivo de texto para entrenar el modelo")

uploaded_file = st.file_uploader("Sube un archivo PDF o TXT", type=["pdf", "txt"])

if uploaded_file is not None:
    if uploaded_file.type == "application/pdf":
        text = extract_text_from_pdf(uploaded_file)
    else:
        text = uploaded_file.getvalue().decode("utf-8")

    with open("training_text.txt", "w") as f:
        f.write(text)

    st.success("‚úÖ Archivo cargado y listo para el entrenamiento.")

# Funci√≥n para entrenar el modelo
def train_model(dataset_path="training_text.txt", output_dir="gpt2_finetuned"):
    st.write("‚ö° Entrenando modelo... Esto puede tardar unos minutos.")

    # Crear dataset
    train_dataset = TextDataset(
        tokenizer=tokenizer,
        file_path=dataset_path,
        block_size=128
    )

    # Configuraci√≥n del entrenamiento
    training_args = TrainingArguments(
        output_dir=output_dir,
        overwrite_output_dir=True,
        num_train_epochs=3,
        per_device_train_batch_size=2,
        save_steps=500,
        save_total_limit=1,
        prediction_loss_only=True
    )

    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False
    )

    trainer = Trainer(
        model=AutoModelForCausalLM.from_pretrained(MODEL_NAME),
        args=training_args,
        train_dataset=train_dataset,
        data_collator=data_collator
    )

    trainer.train()
    trainer.save_model(output_dir)
    st.success("üéâ ¬°Entrenamiento completado! Modelo guardado en 'gpt2_finetuned'.")

# Bot√≥n para entrenar el modelo solo si hay un archivo cargado
if uploaded_file is not None and st.button("Entrenar Modelo"):
    train_model()

# **Secci√≥n de generaci√≥n de texto**
st.subheader("üìù Generaci√≥n de Texto")
prompt = st.text_area("Introduce tu prompt:", "Hab√≠a una vez en un mundo futurista...")

# Funci√≥n para generar texto
def generate_text(prompt, model, tokenizer, max_length=100, temperature=0.7, top_k=50, top_p=0.9):
    input_ids = tokenizer.encode(prompt, return_tensors="pt")

    output = model.generate(
        input_ids,
        max_length=max_length,
        temperature=temperature,  # Controla la creatividad
        top_k=top_k,              # Filtra palabras poco probables
        top_p=top_p,              # Generaci√≥n m√°s coherente
        repetition_penalty=1.2,    # Evita repeticiones
        do_sample=True
    )

    return tokenizer.decode(output[0], skip_special_tokens=True)

# Bot√≥n para generar texto
if st.button("Generar Texto"):
    with st.spinner("‚úçÔ∏è Generando texto..."):
        generated_text = generate_text(prompt, model, tokenizer)
        st.write("### ‚ú® Texto generado:")
        st.write(generated_text)
